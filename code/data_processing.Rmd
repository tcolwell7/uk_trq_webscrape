---
title: "UK TRQ web-scrape timeseries"
output: html_document
---

```{r}
# Set up -------------------------

rm(list=ls()) # remove everything form global environment.

library(tidyverse)
library(openxlsx) 
library(readxl)
library(janitor)
library(stringr)
library(rvest)# most common web-scraping R package. (and very easy to pick up!)
library(tictoc) # simple function to monitor code chunk run time
library(data.table)
library(plotly)
library(tictoc)
library(lubridate)

path<-setwd(stringr::str_extract(rstudioapi::getActiveDocumentContext()$path,".+[/]"))# set working directory to where your file is saved. 

`%notin%` <- Negate(`%in%`) 

```


Define webpage to scrape all weblinks which contain the csv files and dates. There is one date per csv file. Each date can be used to iterate through the data. These dates will be used for the time-stamp to create the time-series. 


```{r}

url <- "https://www.data.gov.uk/dataset/4a478c7e-16c7-4c28-ab9b-967bb79342e9/uk-trade-quotas"

# scrape all web-links. 
scrape <- 
  read_html(url) %>%
  html_nodes(".govuk-link") %>%
  html_attr("href")

# filter for .csv links only. 
scrape_csv <- scrape[grepl("csv",scrape)]

# there is an issue in the web-site where links have been duplicated for the same date
# manual removal of these links:

scrape_csv = scrape_csv[-c(11:19,23:25,27:32)]

# scrape date of each scrapped web-link. 
scrape_dates <- 
  read_html(url) %>%
  html_nodes(".govuk-table__cell") %>%
  html_text() %>%
  stringr::str_squish()

text_search <- ("January|February|March|April|May|June|July|August|September|October|November|December")

# identify dates and remove duplicates. One date-per csv file scraped. 
dates <- unique(scrape_dates[grepl(text_search,scrape_dates)])

```



```{r}
# create function to scrape data
# data.table used to increase speed of run time

get_uk_trqData <- function(href,date){
  
print(href)
print(date)
options(warn=-1) # turn off warning messages

df <- setDT(read.csv(paste0(href,".csv")))
# date columns to convert
dateCols = c("quota_definition__validity_start_date",
             "quota_definition__validity_end_date",
             "quota_definition__last_allocation_date")
# columns to select
keepCols = c("quota__order_number","quota__geographical_areas",
             "quota__measurement_unit","quota_definition__validity_start_date",
             "quota_definition__validity_end_date",
             "quota_definition__last_allocation_date",
             "quota_definition__status",
             "quota_definition__initial_volume",
             "quota_definition__balance",
             "quota_definition__fill_rate")

df2 <- 
  df[, quota__order_number := as.character(paste0("0",quota__order_number))
    ][, (dateCols) := lapply(.SD, FUN = ymd), .SDcols = (dateCols)
    ][, quota_definition__balance := as.numeric(quota_definition__balance)
    ][, (keepCols), with = FALSE
    ][, `:=`(data_date = date, web_link = href)] # add items from scrape to each data table. 


}

```



```{r}

# running all web links takes several minutes
tic()
full_data <-
  mapply(
    FUN=get_uk_trqData,
    href=scrape_csv, 
    date=dates, 
    SIMPLIFY = FALSE # simplify = TRUE returns a matrix. 
    ) 

toc()

full_data2 <- data.table::rbindlist(full_data)

```
