---
title: "UK TRQ web-scrape timeseries"
output: html_document
---

```{r}
# Set up -------------------------

rm(list=ls()) # remove everything form global environment.

library(tidyverse)
library(openxlsx) 
library(readxl)
library(janitor)
library(stringr)
library(rvest)# most common web-scraping R package. (and very easy to pick up!)
library(tictoc) # simple function to monitor code chunk run time
library(data.table)
library(plotly)
library(tictoc)
library(lubridate)

path<-setwd(stringr::str_extract(rstudioapi::getActiveDocumentContext()$path,".+[/]"))# set working directory to where your file is saved. 

`%notin%` <- Negate(`%in%`) 

```


# Scrape UK TRQ data 

Define webpage to scrape all weblinks which contain the csv files and dates. There is one date per csv file. Each date can be used to iterate through the data. These dates will be used for the time-stamp to create the time-series. 


```{r}

url <- "https://www.data.gov.uk/dataset/4a478c7e-16c7-4c28-ab9b-967bb79342e9/uk-trade-quotas"

# scrape all web-links. 
scrape <- 
  read_html(url) %>%
  html_nodes(".govuk-link") %>%
  html_attr("href")

# filter for .csv links only. 
scrape_csv <- scrape[grepl("csv",scrape)]

# there is an issue in the web-site where links have been duplicated for the same date
# manual removal of these links:

scrape_csv = scrape_csv[-c(10:18,22:24,26:31)]

# scrape date of each scrapped web-link. 
scrape_dates <- 
  read_html(url) %>%
  html_nodes(".govuk-table__cell") %>%
  html_text() %>%
  stringr::str_squish()

text_search <- ("January|February|March|April|May|June|July|August|September|October|November|December")

# identify dates and remove duplicates. One date-per csv file scraped. 
dates <- unique(scrape_dates[grepl(text_search,scrape_dates)])

```

## Create function to automate web-scrape of csv files

```{r}
# create function to scrape data
# data.table used to increase speed of run time

get_uk_trqData <- function(href,date){
  
print(href)
print(date)
options(warn=-1) # turn off warning messages

df <- setDT(read.csv(paste0(href,".csv")))
# date columns to convert
dateCols = c("quota_definition__validity_start_date",
             "quota_definition__validity_end_date",
             "quota_definition__last_allocation_date")
# columns to select
keepCols = c("quota__order_number","quota__geographical_areas",
             "quota__measurement_unit","quota_definition__validity_start_date",
             "quota_definition__validity_end_date",
             "quota_definition__last_allocation_date",
             "quota_definition__status",
             "quota_definition__initial_volume",
             "quota_definition__balance",
             "quota_definition__fill_rate")

df2 <- 
  df[, quota__order_number := as.character(paste0("0",quota__order_number))
    ][, (dateCols) := lapply(.SD, FUN = ymd), .SDcols = (dateCols) # convert date columns
    ][, quota_definition__balance := as.numeric(quota_definition__balance)
    ][, (keepCols), with = FALSE
    ][, `:=`(data_date = dmy(date), web_link = href)] # add items from scrape to each data table. 


}

```


### Apply funciton and scrape all TRQ data

```{r}

# running all web links takes several minutes
tic()
full_data <-
  mapply(
    FUN=get_uk_trqData,
    href=scrape_csv, 
    date=dates, 
    SIMPLIFY = FALSE # simplify = TRUE returns a matrix. 
    ) 

toc()

# full scraped data as df 
full_data2 <- data.table::rbindlist(full_data)

# save data

dte <- Sys.Date()
write.xlsx(full_data2, paste0("outputs/full_data_", dte, ".xlsx"),overwrite = TRUE)


```


# Data cleaning 

The data compiled contains all quota data across all quotas and their fill-rate state at the time the file was created, thus creating a time series of fill rates. 

There are a couple of issues to work through to create a more complete data set. First, we want a year-long time series. The data published is sporadic and not daily, for example it can be updated every other week. Therefore we need to create daily data for the full calendar year. The second issue is with these newly create days, there are gaps as no data exists: these need to be filled using either an average or the latest the data closest to the date is used as a substitute. 


Need to calculate each individual quota length and the data date distance away from the quota start date - this is in order to establish what day the fill rate is after the quota opens. i.e. how filled is the quota 20 days, 40 days after the quota opens etc. 


```{r}

# filter for 2022 data only - which is the quota year with a full calendar years worth of data

data22 <-
  full_data2[,`:=`(
                    yr = year(quota_definition__validity_start_date),
                    yr_data = year(data_date)
                  )
            ][yr == 2022 & quota_definition__status != "Future"
            ][, `:=`(
                     quota_length = # quota length in days
                        as.numeric(
                         difftime(
                           quota_definition__validity_end_date,
                           quota_definition__validity_start_date,
                           units =c("days")
                        )
                      ),
                     datacut_day_diff = # difference in days data was extracted to quota start
                       as.numeric(
                        difftime( 
                          data_date,
                          quota_definition__validity_start_date,
                          units = c("days")
                       )
                    )
                  )
             ] 

```


We require a dataset for each quota per quota year and the corresponding fill rates each day - after the quota is open for the time series. This can be done by creating each individual quota data set individually, based on each unique quota length and iterated through the entire quota list. 



```{r}

quota_list <- data22 %>% 
  select(quota__order_number) %>%
  distinct() %>%
  pull()


create_timeSeries <- function(quota){
  
  #quota <- "050006"
  print(quota)
  quota_len = data22 %>% 
    filter(quota__order_number ==quota) %>% 
    select(quota_length) %>%
    distinct(quota_length) %>%
    pull()
    
  tryCatch({ # error function for quotas with multiple quota periods (solution tbc). 
  
  dt <- data.table(quota_number = quota,
                   dayno = seq(1,quota_len,1))
  
  dt <- dt %>% left_join(
    data22,
    by = c("dayno"="datacut_day_diff",
           "quota_number"="quota__order_number")
  )
  
  
  quota_start = dt %>% # quota period start date
    filter(!is.na(quota_definition__validity_start_date)) %>%
    select(quota_definition__validity_start_date) %>% distinct() %>% pull()
  
  dt2 <- dt %>% arrange(-dayno) %>%
    fill(everything()) %>% # fill all blank rows with rows above
    {if(quota_start == "2022-01-01")
      mutate(.,
          quota_definition__fill_rate = # data on website is incorrect for first month
             ifelse( # therefore set all data to 0 for this period until fix is implemented
               dayno < 28,
               0,
               quota_definition__fill_rate
             )
           )
         else .}
  
  
  dt2$val.percentile <- ecdf(dt2$dayno)(dt2$dayno)
  
  return(dt2)
  
  } , error = function(e) { skip_to_next <<- TRUE}) 
  # if code executed within tr catch breaks
  # error function skips to next iteration
  
  
  
}


df <- lapply(quota_list,create_timeSeries)
df<-df[sapply(df, class) != "logical"] %>%rbindlist(use.names=TRUE) # remove errors from list. 


# match in metadata for output
metadata <- read_excel("inputs/uk_trq_mapping_output.xlsx") %>% clean_names()

time_series22 <- df %>% left_join(metadata, by = c("quota_number"="order_number"))


```


Data has now been extracted and compiled together so for each individual TRQ a corresponding fill-rate is assigned to each day the quota is open, based on the fill-rate from the closet date in which the TRQ fill rates were published. 

there are two issues to resolve, the first is data issues remain. For quotas starting later in the year (November/October) the data has incorrectly been assigned as fully filled when the quota opens - this is incorrect. Therefore manual fix is required. Second is to identify all quotas in original list which data has not been compiled, i.e. for quota with multiple periods for further exploration at a later date. 




```{r}


# having analyses the examples where quotas are filled within the first few days
# there are only a select number of quotas which this is an issue. 

manual_fix_quotas <- c("051104","051137")

time_series22 <- 
  time_series22 %>% 
  mutate(
    quota_definition__fill_rate =
      ifelse(
        (quota_number %in% manual_fix_quotas) &
        (val.percentile <= 0.05 & quota_definition__fill_rate > 0.1),
        0,
        quota_definition__fill_rate
      )
  )


missing_quota_list <-
  data22 %>%
  filter(quota__order_number %notin% {unique(time_series22$quota_number)}) %>%
  distinct(quota__order_number, .keep_all = TRUE)
  

# save output
save(
  time_series22, # time series of 2022 quota year data
  data22, # full data scrape of 2022 data used to create time series
  missing_quota_list, # missing data form time series due to multi-quota periods
  file = paste0("outputs/trq_time_series_",Sys.Date(),".RData")
  )



```


# End. 


